{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"ShannonStore Documentation"},{"location":"architecture/architecture/","title":"Architecture","text":"<p>ShannonStore is a high-performance, S3-compatible object storage system designed for high availability and elastic scalability in modern data environments.</p>"},{"location":"architecture/architecture/#shannonstore-architecture","title":"ShannonStore Architecture","text":"<p>Control &amp; Logic Layer (API Servers) is the \"Brain\" of ShannonStore. It interprets the S3 protocol and orchestrates data movement.</p> <ul> <li>S3 Protocol Handling: Processes RESTful requests and generates S3-compliant XML/JSON responses.</li> <li>Metadata Management (RocksDB): Manages object locations, sizes, and permissions. Metadata is logically divided into 64+ Partitions and distributed across the API cluster.</li> <li>Security (KMS &amp; IAM): Enforces granular Role-Based Access Control (RBAC) and ensures all data is encrypted at rest(SSE-S3, SSE-KMS support) and in transit using a distributed Key Management Service.</li> <li>Request Coordination:<ul> <li>Writes: Encodes incoming data into Erasure Coding (EC) shards (e.g., 2+1 or 4+2) and transmits them in parallel to Data Nodes.</li> <li>Reads: Retrieves shards from multiple Data Nodes and reconstructs the original data via EC decoding.</li> </ul> </li> </ul> <p>Storage Layer (Data Nodes) is the \"Warehouse\" of the cluster, responsible for persisting the actual data fragments (chunks).</p> <ul> <li>Chunk Store: Physically stores binary fragments received from API servers on local disks.</li> <li>High-Speed I/O: Utilizes NIO (Non-blocking I/O) communication to handle high-throughput, large-scale data transfers efficiently.</li> <li>Agnostic Storage: Data nodes are \"metadata-blind.\" They focus solely on storing and retrieving bytes associated with a Chunk ID, which maximizes the system\u2019s horizontal scalability.</li> </ul> <p>Coordination Layer (ZooKeeper) is the \"Nervous System\" that maintains cluster consensus and global state.</p> <ul> <li>Service Discovery: Tracks the real-time availability of all API and Data nodes.</li> <li>Leader Election: Elects a Cluster Controller (Leader API node) responsible for partition assignments and centralized action history.</li> <li>Sticky Partitioning: Persists the Partition Assignment Map. This ensures that even after a restart, partitions remain mapped to the same nodes, preventing unnecessary data migration.</li> </ul> <p>ShannonStore has the following architectural advantages.</p> <ul> <li>Independent Scalability: Storage capacity (Data Nodes) and processing power (API Servers) can be scaled independently.</li> <li>Fault Tolerance: The system survives Data Node failures via Erasure Coding and API Node failures via ZooKeeper-based failover.</li> <li>Recoverable State: All critical states (IAM, KMS, History) are backed up to the Data Nodes, making API servers functionally stateless and easily replaceable.</li> </ul>"},{"location":"features/auth-authz/","title":"Authentication &amp; Authorization","text":""},{"location":"features/auth-authz/#admin-uiapi-authentication","title":"Admin UI/API Authentication","text":"<ul> <li>JWT-based authentication with short-lived access tokens and longer-lived refresh tokens.</li> <li>Supports explicit logout with token revocation.</li> </ul>"},{"location":"features/auth-authz/#s3-api-authentication","title":"S3 API Authentication","text":"<ul> <li>Parses AWS Signature V4 and V2 headers to extract and validate access keys.</li> <li>On validation failure, attempts a cluster-wide IAM reload before rejecting the request.</li> </ul>"},{"location":"features/auth-authz/#s3-authorization-flow","title":"S3 Authorization Flow","text":"<ul> <li>Each S3 request is mapped to an action and resource ARN, then evaluated against all applicable IAM policies (user-attached + group-inherited).</li> <li>Explicit Deny always wins; if no Allow is found, access is denied by default.</li> </ul>"},{"location":"features/backup/","title":"Backup &amp; Restore","text":"<p>ShannonStore provides comprehensive backup and restore for metadata, IAM state, and encryption keys.</p> <ul> <li>Metadata Backup: Each API Node exports its metadata partitions as compressed data, distributed to Data Nodes with configurable replication.</li> <li>IAM State Backup: All users, groups, policies, and access keys are serialized, encrypted, and stored on Data Nodes.</li> <li>KMS Keystore Backup: Encrypted keystores are backed up to Data Nodes after key rotation events.</li> </ul>"},{"location":"features/backup/#restore","title":"Restore","text":"<ul> <li>Backups are downloaded from Data Nodes, imported into local stores, and in-memory caches are rebuilt.</li> <li>Restored metadata is replicated to replica nodes for consistency.</li> </ul>"},{"location":"features/backup/#smart-rebalance","title":"Smart Rebalance","text":"<p>A 5-phase orchestrated workflow for safe partition rebalancing: enable maintenance mode \u2192 flush buffers \u2192 backup metadata \u2192 rebalance partitions \u2192 restore and resume operations.</p>"},{"location":"features/configuration/","title":"Configuration Management","text":"<p>ShannonStore supports flexible configuration through a priority-based resolution chain.</p> <ul> <li>Priority order (highest to lowest): System properties \u2192 Environment variables \u2192 External config file \u2192 Default classpath properties.</li> <li>Supports variable substitution (<code>${variable}</code> syntax) for DRY configuration.</li> <li>Over 150 configurable parameters covering cluster connectivity, storage, security, performance tuning, and more.</li> </ul>"},{"location":"features/data-compression/","title":"Data Compression","text":"<p>ShannonStore supports multiple compression algorithms to reduce storage costs and network bandwidth.</p> <ul> <li>Supported algorithms: SNAPPY (speed-optimized), GZIP (higher ratio for text), ZSTD (best overall ratio), or NONE.</li> <li>Compression is selectively applied based on MIME type \u2014 binary formats like images and video are automatically skipped.</li> <li>Auto-selection mode samples data content to choose the best algorithm automatically.</li> <li>Network compression is applied separately for inter-node transfers above a configurable threshold.</li> </ul>"},{"location":"features/disk-repair/","title":"Disk Repair Service","text":"<p>ShannonStore includes an automatic disk repair system that detects failed disks and reconstructs lost shards using erasure coding.</p> <ul> <li>Periodically scans metadata to identify shards on disks that are no longer healthy, with a grace period to avoid reacting to transient issues.</li> <li>Surviving shards are fetched from healthy nodes, missing shards are reconstructed via erasure coding, and repaired shards are placed on new healthy disks.</li> <li>Metadata is updated to reflect the new shard locations and replicated to ensure consistency.</li> <li>Configurable scan interval, concurrency limits, and minimum disk space thresholds. Manual trigger is also available via Admin UI or API.</li> </ul>"},{"location":"features/distributed-architecture/","title":"Distributed Architecture","text":"<p>ShannonStore follows a disaggregated architecture with clear separation between control plane and data plane.</p>"},{"location":"features/distributed-architecture/#api-nodes-control-plane","title":"API Nodes (Control Plane)","text":"<ul> <li>Handle S3 protocol requests, manage object metadata, and coordinate cluster operations (leader election, partition assignment, backup/restore).</li> <li>Perform erasure coding, encryption, and compression before distributing data to Data Nodes.</li> <li>Serve the Admin UI and Admin API on a dedicated port.</li> </ul>"},{"location":"features/distributed-architecture/#data-nodes-data-plane","title":"Data Nodes (Data Plane)","text":"<ul> <li>Store raw data chunks (erasure-coded shards) on local disks.</li> <li>Serve chunk read/write requests from API Nodes and report disk health to the cluster.</li> <li>Support multi-disk configurations for scalable storage capacity.</li> </ul>"},{"location":"features/distributed-architecture/#two-port-architecture","title":"Two-Port Architecture","text":"<ul> <li>S3 Port (default 8080): High-throughput object storage operations.</li> <li>Admin Port (default 8888): Admin UI, REST API, and cluster management. Separated to avoid contention with the data path.</li> </ul>"},{"location":"features/ec/","title":"Erasure Coding","text":"<p>ShannonStore uses Reed-Solomon erasure coding to protect data against hardware failures with minimal storage overhead.</p> <ul> <li>Objects are split into data shards and parity shards. Even if some shards are lost, the original data can be fully recovered.</li> <li>Default configuration tolerates up to 2 simultaneous shard failures with only 50% storage overhead, compared to 200\u2013300% for traditional replication.</li> <li>Shards are automatically spread across different nodes and disks to maximize fault tolerance.</li> <li>When failures occur, only the missing shards are regenerated \u2014 no need to reassemble the entire object.</li> </ul>"},{"location":"features/iam/","title":"Identity &amp; Access Management (IAM)","text":"<p>ShannonStore provides an AWS IAM-compatible access control system.</p> <ul> <li>Users and groups can be managed through the Admin UI, with policies controlling what each user or group is allowed to do.</li> <li>Policies follow the standard AWS IAM JSON format, supporting wildcard matching and explicit Deny-over-Allow evaluation.</li> <li>Access key pairs can be generated for programmatic S3 API access, with support for expiration and status management.</li> <li>On first cluster start, a default admin user, group, and full-access policy are automatically created.</li> </ul>"},{"location":"features/kms/","title":"Encryption &amp; Key Management (KMS)","text":"<p>ShannonStore provides server-side encryption with a built-in distributed key management system.</p> <ul> <li>Uses envelope encryption: data is encrypted with a per-object key, which is itself encrypted by a master-derived key (AES-256-GCM).</li> <li>The master key is stored as an environment variable and never written to config files.</li> <li>Key rotation is supported \u2014 new keys are created for future encryption while old keys remain available for decrypting existing data.</li> <li>Key state is automatically synchronized across all cluster nodes by the leader.</li> </ul>"},{"location":"features/maintenance/","title":"Maintenance Mode","text":"<p>ShannonStore supports a global maintenance mode for safe cluster-wide operations.</p> <ul> <li>When activated, all S3 API requests are temporarily rejected while admin operations remain available.</li> <li>Used during partition rebalancing, cluster-wide backups, and other operations that require consistent cluster state.</li> </ul>"},{"location":"features/metadata-management/","title":"Metadata Management","text":"<p>ShannonStore manages object metadata through a partitioned, replicated, and cached architecture.</p> <ul> <li>Metadata is stored in RocksDB and distributed across configurable partitions, each with a primary node and replicas for fault tolerance.</li> <li>A two-tier caching layer (in-memory + RocksDB block cache) provides fast lookups without disk I/O for frequently accessed objects.</li> <li>Partition assignments are automatically rebalanced when nodes join or leave the cluster.</li> <li>Writes replicate asynchronously to replicas; reads follow a fallback chain from local cache to primary to replicas.</li> </ul>"},{"location":"features/monitoring/","title":"Monitoring &amp; Metrics","text":"<p>ShannonStore provides built-in monitoring with Prometheus integration.</p> <ul> <li>Exposes a <code>/metrics</code> endpoint for Prometheus scraping, covering S3 operation counts and latencies.</li> <li>Collects system metrics (CPU, memory, disk usage) and S3 API statistics from all nodes at configurable intervals.</li> <li>Metrics are stored internally with configurable retention (default: 7 days) and automatic cleanup.</li> <li>All stored metrics are encrypted at rest.</li> </ul>"},{"location":"features/multi-disk-storage/","title":"Multi-Disk Storage","text":"<p>ShannonStore Data Nodes support multiple local disks for scalable storage capacity.</p> <ul> <li>Chunks are evenly distributed across disks using hash-based placement for balanced utilization.</li> <li>Read and write operations include fallback mechanisms to handle disk topology changes gracefully.</li> <li>Disk health (capacity, usage, availability) is continuously tracked and reported to the cluster for placement decisions.</li> </ul>"},{"location":"features/network/","title":"Network &amp; Internal Communication","text":"<p>ShannonStore nodes communicate via a custom binary NIO RPC protocol.</p> <ul> <li>A lightweight binary protocol with request-reply correlation enables efficient concurrent operations between nodes.</li> <li>Supports transparent network compression for payloads above a configurable threshold.</li> <li>TCP socket parameters are tunable for optimal throughput and low latency.</li> </ul>"},{"location":"features/performance/","title":"Performance Optimization","text":"<p>ShannonStore is designed for high throughput and low latency through several optimization strategies.</p> <ul> <li>Part Buffering: Multipart uploads use a dual-layer buffer (in-memory + disk overflow) with encryption, and parts are pre-assigned to erasure coding shards to avoid re-encoding on completion.</li> <li>Quorum Writes: PUT operations return success as soon as enough data shards acknowledge, while parity shards continue writing in the background.</li> <li>Streaming Prefetch: Large object downloads use async prefetching to eliminate blocking while the client reads data.</li> <li>Connection Pooling: Persistent connections to Data Nodes are pooled and reused for efficient RPC communication.</li> <li>Thread Pool Isolation: Separate thread pools for S3 requests, chunk operations, encoding, and background tasks prevent resource contention.</li> </ul>"},{"location":"features/s3-compatible-api/","title":"S3-Compatible API","text":"<p>ShannonStore provides a fully compatible Amazon S3 REST API, allowing applications to use standard S3 SDKs and tools (AWS CLI, boto3, MinIO client, etc.) without any code changes.</p>"},{"location":"features/s3-compatible-api/#supported-operations","title":"Supported Operations","text":"<ul> <li>Object Operations: PutObject, GetObject, HeadObject, DeleteObject \u2014 full CRUD lifecycle with support for key prefixes simulating directory structures.</li> <li>Bucket Operations: CreateBucket, DeleteBucket, ListBuckets, ListObjects \u2014 bucket management with AWS-compatible XML responses.</li> <li>Multipart Upload: Upload large objects in parallel parts, assembled on completion.</li> <li>Object Versioning: Per-bucket versioning with unique version IDs, version history, and delete markers.</li> <li>Range Requests: Partial object downloads for efficient large file access and resumable downloads.</li> <li>AWS Authentication: Supports both AWS Signature V4 and legacy V2 authentication.</li> <li>Chunked Transfer Encoding: Supports AWS SDK streaming upload format.</li> </ul>"},{"location":"features/s3-compatible-api/#server-architecture","title":"Server Architecture","text":"<p>The S3 API runs on a custom NIO HTTP server with a two-port design: - S3 Port (default 8080): Optimized for high-throughput object storage operations. - Admin Port (default 8888): Serves the Admin UI, REST API, and cluster management endpoints.</p>"},{"location":"intro/intro/","title":"Getting Started","text":"<p>This shows how to install ShannonStore on local to experience it quickly.</p>"},{"location":"intro/intro/#prerequisites","title":"Prerequisites","text":"<p>Because ShannonStore is written in Java, Java 17 needs to be installed on local.</p>"},{"location":"intro/intro/#install-shannonstore-on-local","title":"Install ShannonStore on Local","text":"<p>ShannonStore distribution can be downloaded like this.</p> <pre><code>curl -L -O https://github.com/cloudcheflabs/shannonstore-pack/releases/download/shannonstore-archive/shannonstore-1.0.0.tar.gz\n</code></pre> <p>And untar the downloaded package. <pre><code>tar zxvf shannonstore-1.0.0.tar.gz\n\ncd shannonstore-1.0.0;\n</code></pre></p> <p>Run the example servers which are 1 API server and 3 Data nodes with Zookeeper on local.</p> <pre><code>export SHANNONSTORE_MASTER_KEY=\"ShannonStoreMasterKey1200303003abc\"\nbin/start-example-servers.sh;\n</code></pre> <p>Environment variable <code>SHANNONSTORE_MASTER_KEY</code> that must be at least 32 characters needs to be exported when running ShannonStore servers.</p> <p>After a few seconds, visit admin page of ShannonStore.</p> <pre><code>http://localhost:8888/admin\n</code></pre> <p>First initial admin user and password is <code>admin</code> / <code>admin</code>, after that you need to change the initial password.</p> <p></p>"},{"location":"intro/intro/#create-bucket-and-credential","title":"Create Bucket and Credential","text":"<p>In order to upload files to ShannonStore, a bucket and a S3 credential need to be created in Admin UI.</p> <p>In the menu of <code>Identity Control</code>, click <code>New Keypair</code> to create a S3 credential(access key / secret key). After that, click the download icon in the created keypair to download access key and secret key. </p> <p>And you also need to create a bucket, go to the menu of <code>Object Store</code>, click <code>New Bucket</code> to create a bucket with the name of <code>test-bucket</code>.</p>"},{"location":"intro/intro/#upload-files-to-shannonstore","title":"Upload files to ShannonStore","text":"<p>Now, you can connect ShannonStore to upload files. AWS CLI will be used for this example. Configure a S3 profile with the S3 credential(access key and secret key) from the downloaded credential file before.</p> <pre><code>aws configure --profile=custom-s3 set default.s3.signature_version s3v4;\naws configure --profile=custom-s3 set aws_access_key_id &lt;access-key&gt;;\naws configure --profile=custom-s3 set aws_secret_access_key &lt;secret-key&gt;;\naws configure --profile=custom-s3 set region us-east-1;\n</code></pre> <p>Upload a file to ShannonStore as below, take a note that the S3 endpoint is <code>http://localhost:8080</code>.</p> <pre><code># upload file.\naws s3 cp ./values-dev_old.yaml s3://test-bucket/values-dev_old.yaml \\\n--profile=custom-s3 \\\n--endpoint-url http://localhost:8080 \\\n--no-verify-ssl\n</code></pre> <p>You can also test <code>ls</code>, <code>download</code>, <code>delete</code> of objects in ShannonStore. <pre><code># ls.\naws s3 ls \\\ns3://test-bucket \\\n--profile=custom-s3 \\\n--endpoint=http://localhost:8080 \\\n--no-verify-ssl \\\n--recursive \\\n--human-readable \\\n--summarize \\\n;\n\n\n# download.\naws s3 cp \\\n--profile=custom-s3 \\\n--endpoint=http://localhost:8080 \\\ns3://test-bucket/values-dev_old.yaml values-dev_downloaded.yaml \\\n;\n\n# delete.\naws s3 rm \\\n--profile=custom-s3 \\\n--endpoint=http://localhost:8080 \\\ns3://test-bucket/values-dev_old.yaml \\\n;\n</code></pre></p>"},{"location":"intro/intro/#stop-example-servers","title":"Stop Example Servers","text":"<p>In order to stop the running example servers, run this.</p> <pre><code>bin/stop-example-servers.sh\n</code></pre>"}]}