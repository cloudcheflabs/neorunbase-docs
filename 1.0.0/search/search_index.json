{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"NeorunBase Documentation"},{"location":"architecture/architecture/","title":"Architecture","text":"<p>NeorunBase is a distributed, sharded OLTP Lakebase that implements the PostgreSQL wire protocol. Any PostgreSQL-compatible client such as <code>psql</code>, JDBC drivers, or pgAdmin can connect to NeorunBase transparently without any modification.</p> <p>NeorunBase provides horizontal scalability through hash-based sharding across multiple Data Nodes, fault tolerance via shard replication, and encryption at rest using envelope encryption with a built-in KMS. It also integrates with Apache Iceberg,  enabling automatic synchronization of transactional data to an open lakehouse format for downstream analytics by engines such as Apache Spark, Trino, and Hive. Additionally, NeorunBase supports streaming ingestion from Apache Kafka, allowing real-time data to flow directly into NeorunBase tables.</p>"},{"location":"architecture/architecture/#neorunbase-architecture","title":"NeorunBase Architecture","text":"<p>NeorunBase consists of two main deployable components: Coordinator and Data Node.</p>"},{"location":"architecture/architecture/#coordinator","title":"Coordinator","text":"<p>The Coordinator is the SQL-facing entry point that clients connect to via the PostgreSQL wire protocol. It is responsible for:</p> <ul> <li>SQL Parsing: Parses incoming SQL statements using Apache Calcite (for DML) and a custom DDL parser.</li> <li>Query Routing: Determines the target shards using Murmur3 hash-based shard routing on the shard key, and prunes shards with Bloom filter caches.</li> <li>Distributed Query Execution: Scatters queries to the relevant Data Nodes in parallel via an internal binary NIO protocol (with Snappy compression and AES encryption), then merges partial results (sort-merge, aggregation, LIMIT).</li> <li>Distributed Transactions: Supports ACID transactions across shards using a two-phase commit protocol (PREPARE + COMMIT).</li> <li>Cluster Metadata Management: The leader Coordinator maintains table schemas and shard maps in an encrypted RocksDB-backed metadata store, and synchronizes them to non-leader Coordinators.</li> <li>Admin API: Exposes a Netty-based REST API and serves the React-based Admin UI for cluster management, IAM, metrics monitoring, and operational tasks.</li> </ul>"},{"location":"architecture/architecture/#data-node","title":"Data Node","text":"<p>The Data Node is the storage layer of NeorunBase. It is responsible for:</p> <ul> <li>Shard Storage: Each shard is backed by a RocksDB <code>TransactionDB</code> instance with per-table column families, supporting data-at-rest encryption via envelope encryption (per-shard DEK).</li> <li>Query Execution: Processes DML operations (INSERT, UPDATE, DELETE, SELECT) on local shards, including index scans and predicate pushdown.</li> <li>Write-Ahead Log (WAL): Maintains encrypted, segmented WAL for durability.</li> <li>Change Log: Records data changes for incremental Iceberg synchronization.</li> </ul>"},{"location":"architecture/architecture/#cluster-coordination","title":"Cluster Coordination","text":"<p>NeorunBase uses Apache ZooKeeper (via Curator) for:</p> <ul> <li>Service Discovery: Coordinators and Data Nodes register as ephemeral nodes, enabling automatic detection of node joins and failures.</li> <li>Leader Election: Two separate leader elections \u2014 one for shard assignment (Controller) and one for metadata/KMS ownership (Coordinator Leader).</li> <li>Shard Repair &amp; Rebalancing: Automatically detects dead Data Nodes and replicates shard replicas to healthy nodes; rebalances shards when nodes are added or removed.</li> </ul>"},{"location":"architecture/architecture/#iceberg-integration","title":"Iceberg Integration","text":"<p>NeorunBase synchronizes table data to Apache Iceberg for open lakehouse analytics:</p> <ul> <li>Connects to an Iceberg REST catalog (e.g., Polaris) with OAuth2 or static token authentication.</li> <li>Performs full snapshot sync on initial synchronization, followed by incremental sync via change logs using Iceberg RowDelta (equality deletes + data files).</li> <li>Writes Parquet files to S3-compatible object storage.</li> <li>Supports reading external Iceberg tables via distributed Parquet/ORC/Avro scan directly from S3.</li> </ul>"},{"location":"architecture/architecture/#kafka-integration","title":"Kafka Integration","text":"<p>NeorunBase supports streaming data ingestion from Apache Kafka:</p> <ul> <li>Manages multiple Kafka consumer groups, each running as an independent consumer thread.</li> <li>Consumes JSON messages from Kafka topics and batch-inserts them into NeorunBase tables.</li> </ul>"},{"location":"features/admin-ui/","title":"Admin UI","text":"<p>NeorunBase includes a built-in web-based Admin UI for monitoring and managing the cluster.</p>"},{"location":"features/admin-ui/#cluster-overview","title":"Cluster Overview","text":"<p>The Admin UI provides a visual overview of the cluster, including:</p> <ul> <li>List of active Coordinators and Data Nodes</li> <li>Node status and health information</li> <li>Shard distribution across Data Nodes</li> </ul>"},{"location":"features/admin-ui/#monitoring","title":"Monitoring","text":"<p>Real-time metrics and monitoring capabilities:</p> <ul> <li>Query throughput and latency metrics</li> <li>Storage usage per node and per table</li> <li>Cluster-wide performance dashboards</li> <li>Time-series metrics with configurable retention</li> </ul>"},{"location":"features/admin-ui/#management","title":"Management","text":"<p>The Admin UI allows administrators to perform operational tasks:</p> <ul> <li>IAM Management: Create and manage users, groups, and policies</li> <li>Iceberg Sync: Configure and monitor Iceberg synchronization</li> <li>Kafka Ingestion: Manage Kafka consumer groups and monitor ingestion pipelines</li> <li>Shard Operations: Monitor shard health, replication status, and repair progress</li> </ul>"},{"location":"features/admin-ui/#rest-api","title":"REST API","text":"<p>All operations available in the Admin UI are also accessible via a REST API, enabling automation and integration with external monitoring and management tools.</p>"},{"location":"features/distributed-sharding/","title":"Distributed Sharding","text":"<p>NeorunBase distributes table data across multiple Data Nodes through hash-based sharding, providing horizontal scalability for both reads and writes.</p>"},{"location":"features/distributed-sharding/#how-sharding-works","title":"How Sharding Works","text":"<p>When a table is created, NeorunBase assigns a configurable number of shards and distributes them across the available Data Nodes. Each row is assigned to a shard based on a hash of the designated shard key column, ensuring even data distribution.</p>"},{"location":"features/distributed-sharding/#shard-key","title":"Shard Key","text":"<p>The shard key is a column specified at table creation time. NeorunBase uses the shard key value to determine which shard a row belongs to. Queries that include the shard key in their <code>WHERE</code> clause can be routed directly to the relevant shard(s), significantly improving query performance.</p>"},{"location":"features/distributed-sharding/#query-routing","title":"Query Routing","text":"<p>NeorunBase automatically routes queries to the appropriate shards:</p> <ul> <li>Point queries: When the shard key value is specified, the query is routed to a single shard.</li> <li>Range queries: Queries are routed only to the shards that may contain matching data.</li> <li>Full scan queries: When the shard key is not specified, the query is scattered to all shards in parallel and results are merged.</li> </ul>"},{"location":"features/distributed-sharding/#shard-pruning","title":"Shard Pruning","text":"<p>NeorunBase maintains Bloom filter caches for shards, allowing it to skip shards that definitely do not contain the queried values. This reduces unnecessary I/O and improves query latency.</p>"},{"location":"features/distributed-sharding/#resharding","title":"Resharding","text":"<p>NeorunBase supports online resharding, allowing you to change the number of shards for a table. During resharding, data is transparently migrated to the new shard layout without downtime.</p>"},{"location":"features/encryption/","title":"Encryption at Rest","text":"<p>NeorunBase provides built-in encryption at rest to protect data stored on disk, ensuring that sensitive data is always encrypted without any application-level changes.</p>"},{"location":"features/encryption/#envelope-encryption","title":"Envelope Encryption","text":"<p>NeorunBase uses envelope encryption, a widely adopted encryption approach used by major cloud providers. Each shard is encrypted with its own unique Data Encryption Key (DEK), and DEKs are encrypted with a master key managed by the built-in Key Management Service (KMS).</p>"},{"location":"features/encryption/#built-in-kms","title":"Built-in KMS","text":"<p>NeorunBase includes a built-in Key Management Service that:</p> <ul> <li>Generates and manages encryption keys</li> <li>Distributes keys securely across the cluster</li> <li>Synchronizes keys between Coordinators and Data Nodes automatically</li> </ul>"},{"location":"features/encryption/#what-is-encrypted","title":"What Is Encrypted","text":"<ul> <li>Shard data: All table data stored on Data Nodes is encrypted at rest.</li> <li>Metadata: Table schemas and shard maps maintained by the Coordinator are encrypted.</li> <li>Write-Ahead Log (WAL): WAL segments are encrypted to protect data durability records.</li> <li>Internal communication: Data transmitted between Coordinators and Data Nodes is encrypted using AES.</li> </ul>"},{"location":"features/encryption/#transparent-to-clients","title":"Transparent to Clients","text":"<p>Encryption is fully transparent to clients. No changes to queries, connection settings, or application code are required. Data is encrypted when written to disk and decrypted when read, all handled internally by NeorunBase.</p>"},{"location":"features/geospatial/","title":"Geospatial Support","text":"<p>NeorunBase provides PostGIS-compatible geospatial SQL functions, enabling location-based queries and spatial analysis directly within the database.</p>"},{"location":"features/geospatial/#geospatial-data-types","title":"Geospatial Data Types","text":"<p>NeorunBase supports the following geospatial data types:</p> <ul> <li>POINT: A single location in 2D space (longitude, latitude)</li> <li>LINESTRING: A sequence of connected points forming a line</li> <li>POLYGON: A closed shape defined by a series of points</li> <li>GEOMETRY: A generic type that can hold any of the above</li> </ul>"},{"location":"features/geospatial/#spatial-functions","title":"Spatial Functions","text":"<p>NeorunBase includes a set of spatial functions compatible with PostGIS conventions:</p> <ul> <li>Distance: Calculate the distance between two geometries</li> <li>Contains / Within: Check if one geometry contains or is within another</li> <li>Intersects: Determine if two geometries intersect</li> <li>Area / Length: Compute the area of polygons or length of lines</li> <li>Buffer: Create a buffer zone around a geometry</li> <li>And more standard spatial operations</li> </ul>"},{"location":"features/geospatial/#spatial-indexing","title":"Spatial Indexing","text":"<p>NeorunBase uses spatial indexing to accelerate geospatial queries. Spatial indexes enable efficient range queries and proximity searches without scanning the entire dataset.</p>"},{"location":"features/geospatial/#use-cases","title":"Use Cases","text":"<ul> <li>Location-based services: Find points of interest near a given location</li> <li>Geofencing: Determine if a point falls within a defined geographic boundary</li> <li>Spatial analytics: Analyze geographic distribution of data</li> <li>Fleet tracking: Track and query moving objects across regions</li> </ul>"},{"location":"features/iam/","title":"Identity and Access Management","text":"<p>NeorunBase includes a built-in Identity and Access Management (IAM) system that provides authentication, authorization, and fine-grained access control.</p>"},{"location":"features/iam/#user-management","title":"User Management","text":"<p>NeorunBase supports creating and managing database users with:</p> <ul> <li>Username and password-based authentication</li> <li>Access key and secret key credentials for programmatic access</li> <li>Temporary credentials with configurable expiration</li> </ul>"},{"location":"features/iam/#groups","title":"Groups","text":"<p>Users can be organized into IAM groups. Policies attached to a group apply to all members, simplifying access management for teams and applications.</p>"},{"location":"features/iam/#policies","title":"Policies","text":"<p>NeorunBase uses policy-based access control to manage permissions:</p> <ul> <li>Policies define what actions are allowed or denied on specific resources (schemas, tables).</li> <li>Policies can be attached to users or groups.</li> <li>Multiple policies can be combined, with deny rules taking precedence over allow rules.</li> </ul>"},{"location":"features/iam/#admin-api","title":"Admin API","text":"<p>User, group, and policy management is available through the NeorunBase Admin API, allowing administrators to manage access control programmatically or through the Admin UI.</p>"},{"location":"features/iceberg-integration/","title":"Iceberg Integration","text":"<p>NeorunBase integrates with Apache Iceberg, enabling automatic synchronization of transactional data to an open lakehouse format. This allows downstream analytics engines such as Apache Spark, Trino, and Hive to query NeorunBase data directly.</p>"},{"location":"features/iceberg-integration/#automatic-data-synchronization","title":"Automatic Data Synchronization","text":"<p>NeorunBase automatically syncs table data to Iceberg tables in the background:</p> <ul> <li>Initial sync: A full snapshot of the table is exported as Parquet files to S3-compatible object storage and registered in the Iceberg catalog.</li> <li>Incremental sync: After the initial sync, only the changes (inserts, updates, deletes) are synchronized incrementally, minimizing the overhead.</li> </ul>"},{"location":"features/iceberg-integration/#iceberg-catalog-support","title":"Iceberg Catalog Support","text":"<p>NeorunBase connects to any Iceberg REST catalog (e.g., Polaris, Nessie) with support for:</p> <ul> <li>OAuth2 client credentials authentication</li> <li>Static bearer token authentication</li> </ul>"},{"location":"features/iceberg-integration/#open-lakehouse-analytics","title":"Open Lakehouse Analytics","text":"<p>Once data is synced to Iceberg, it can be queried by any engine that supports the Iceberg table format:</p> <ul> <li>Apache Spark: Batch and streaming analytics</li> <li>Trino: Interactive SQL queries</li> <li>Apache Hive: Data warehousing workloads</li> <li>Apache Flink: Stream processing</li> </ul>"},{"location":"features/iceberg-integration/#external-iceberg-table-queries","title":"External Iceberg Table Queries","text":"<p>NeorunBase can also read data from external Iceberg tables. This allows you to query data stored in Iceberg (Parquet, ORC, Avro formats) directly from NeorunBase using standard SQL, bridging the gap between the transactional and analytical worlds.</p>"},{"location":"features/iceberg-integration/#s3-compatible-storage","title":"S3-Compatible Storage","text":"<p>Iceberg data files are stored in S3-compatible object storage, supporting AWS S3, MinIO, and other S3-compatible services.</p>"},{"location":"features/kafka-integration/","title":"Kafka Integration","text":"<p>NeorunBase supports streaming data ingestion from Apache Kafka, enabling real-time data to flow directly into NeorunBase tables.</p>"},{"location":"features/kafka-integration/#streaming-ingestion","title":"Streaming Ingestion","text":"<p>NeorunBase can consume messages from Kafka topics and automatically insert them into designated tables. This provides a seamless pipeline from event streams to a queryable relational database.</p>"},{"location":"features/kafka-integration/#multiple-consumer-groups","title":"Multiple Consumer Groups","text":"<p>You can configure multiple independent Kafka consumer groups, each consuming from different topics and writing to different tables. This allows you to ingest data from various Kafka sources simultaneously.</p>"},{"location":"features/kafka-integration/#configuration","title":"Configuration","text":"<p>Each Kafka consumer group can be configured with:</p> <ul> <li>Kafka broker addresses</li> <li>Topic name</li> <li>Consumer group ID</li> <li>Target NeorunBase table</li> <li>Batch size for efficient bulk inserts</li> <li>Auto offset reset policy</li> <li>Additional Kafka consumer properties</li> </ul>"},{"location":"features/kafka-integration/#json-message-format","title":"JSON Message Format","text":"<p>NeorunBase consumes JSON-formatted messages from Kafka topics. Messages are parsed and batch-inserted into the target table, matching JSON fields to table columns.</p>"},{"location":"features/kafka-integration/#management","title":"Management","text":"<p>Kafka consumer groups can be managed through the NeorunBase Admin API, allowing you to start, stop, and monitor ingestion pipelines at runtime.</p>"},{"location":"features/postgresql-compatibility/","title":"PostgreSQL Compatibility","text":"<p>NeorunBase implements the PostgreSQL wire protocol (v3), allowing any PostgreSQL-compatible client to connect seamlessly without modification.</p>"},{"location":"features/postgresql-compatibility/#supported-clients","title":"Supported Clients","text":"<p>You can connect to NeorunBase using any standard PostgreSQL client, including:</p> <ul> <li><code>psql</code> command-line tool</li> <li>JDBC drivers (PostgreSQL JDBC)</li> <li>pgAdmin</li> <li>Any application or library that supports the PostgreSQL protocol</li> </ul>"},{"location":"features/postgresql-compatibility/#sql-support","title":"SQL Support","text":"<p>NeorunBase supports standard SQL operations with PostgreSQL conformance:</p> <ul> <li>DML: <code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code></li> <li>DDL: <code>CREATE TABLE</code>, <code>DROP TABLE</code>, <code>ALTER TABLE</code>, <code>CREATE INDEX</code>, <code>DROP INDEX</code>, <code>CREATE SCHEMA</code>, <code>DROP SCHEMA</code></li> <li>Transactions: <code>BEGIN</code>, <code>COMMIT</code>, <code>ROLLBACK</code></li> <li>Queries: <code>JOIN</code>, <code>GROUP BY</code>, <code>ORDER BY</code>, <code>LIMIT</code>, <code>HAVING</code>, subqueries, aggregation functions, <code>CASE WHEN</code>, <code>CAST</code>, and more</li> </ul>"},{"location":"features/postgresql-compatibility/#data-types","title":"Data Types","text":"<p>NeorunBase supports a wide range of data types:</p> <ul> <li>Numeric: <code>INTEGER</code>, <code>BIGINT</code>, <code>SMALLINT</code>, <code>FLOAT</code>, <code>DOUBLE</code>, <code>DECIMAL</code></li> <li>String: <code>VARCHAR</code>, <code>TEXT</code>, <code>CHAR</code></li> <li>Temporal: <code>DATE</code>, <code>TIME</code>, <code>TIMESTAMP</code></li> <li>Boolean: <code>BOOLEAN</code></li> <li>Binary: <code>BYTEA</code></li> <li>Geospatial: <code>POINT</code>, <code>LINESTRING</code>, <code>POLYGON</code>, <code>GEOMETRY</code></li> <li>Other: <code>JSON</code>, <code>UUID</code></li> </ul>"},{"location":"features/postgresql-compatibility/#virtual-catalog","title":"Virtual Catalog","text":"<p>NeorunBase implements <code>pg_catalog</code> and <code>information_schema</code> virtual catalogs, enabling standard PostgreSQL introspection commands such as <code>\\d</code>, <code>\\dt</code>, and <code>\\di</code> in <code>psql</code>.</p>"},{"location":"features/replication-ha/","title":"Replication &amp; High Availability","text":"<p>NeorunBase provides fault tolerance and high availability through shard replication and automatic failure recovery.</p>"},{"location":"features/replication-ha/#shard-replication","title":"Shard Replication","text":"<p>Each shard can be configured with a replication factor. NeorunBase maintains multiple copies of each shard across different Data Nodes. This ensures that data remains available even if one or more Data Nodes go down.</p>"},{"location":"features/replication-ha/#automatic-failure-detection","title":"Automatic Failure Detection","text":"<p>NeorunBase continuously monitors the health of all Data Nodes. When a Data Node becomes unavailable, the system automatically detects the failure and initiates recovery actions.</p>"},{"location":"features/replication-ha/#automatic-shard-repair","title":"Automatic Shard Repair","text":"<p>When a Data Node failure is detected, NeorunBase automatically replicates the affected shards from surviving replicas to healthy Data Nodes, restoring the desired replication factor without manual intervention.</p>"},{"location":"features/replication-ha/#shard-rebalancing","title":"Shard Rebalancing","text":"<p>When Data Nodes are added to or removed from the cluster, NeorunBase automatically rebalances shards across the cluster to ensure even data distribution and optimal resource utilization.</p>"},{"location":"features/replication-ha/#coordinator-high-availability","title":"Coordinator High Availability","text":"<p>Multiple Coordinators can run simultaneously. NeorunBase uses leader election to designate a primary Coordinator for metadata management, while all Coordinators can serve client queries. If the leader Coordinator fails, a new leader is automatically elected.</p>"},{"location":"features/transactions/","title":"Distributed Transactions","text":"<p>NeorunBase supports ACID transactions across multiple shards, ensuring data consistency in a distributed environment.</p>"},{"location":"features/transactions/#acid-compliance","title":"ACID Compliance","text":"<p>NeorunBase provides full ACID guarantees for transactions:</p> <ul> <li>Atomicity: All operations within a transaction either succeed together or are rolled back entirely.</li> <li>Consistency: The database transitions from one valid state to another after each transaction.</li> <li>Isolation: Concurrent transactions do not interfere with each other.</li> <li>Durability: Once a transaction is committed, the changes are permanently stored.</li> </ul>"},{"location":"features/transactions/#cross-shard-transactions","title":"Cross-Shard Transactions","text":"<p>When a transaction involves data on multiple shards, NeorunBase coordinates the commit across all participating shards using a two-phase commit protocol:</p> <ol> <li>Prepare phase: All participating shards prepare to commit and confirm readiness.</li> <li>Commit phase: Once all shards confirm, the transaction is committed on all shards simultaneously.</li> </ol> <p>If any shard fails to prepare, the entire transaction is rolled back.</p>"},{"location":"features/transactions/#transaction-support","title":"Transaction Support","text":"<p>NeorunBase supports standard SQL transaction control statements:</p> <ul> <li><code>BEGIN</code> \u2014 Start a new transaction</li> <li><code>COMMIT</code> \u2014 Commit the current transaction</li> <li><code>ROLLBACK</code> \u2014 Roll back the current transaction</li> </ul>"},{"location":"features/transactions/#write-ahead-log","title":"Write-Ahead Log","text":"<p>All write operations are recorded in a Write-Ahead Log (WAL) before being applied to the storage engine. This ensures durability even in the event of a crash \u2014 uncommitted transactions can be recovered from the WAL upon restart.</p>"},{"location":"intro/intro/","title":"Getting Started","text":"<p>This shows how to install NeorunBase on local to experience it quickly.</p>"},{"location":"intro/intro/#prerequisites","title":"Prerequisites","text":"<p>Because NeorunBase is written in Java, Java 17 needs to be installed on local.</p>"},{"location":"intro/intro/#install-neorunbase-on-local","title":"Install NeorunBase on Local","text":"<p>NeorunBase distribution can be downloaded like this.</p> <pre><code>curl -L -O https://github.com/cloudcheflabs/neorunbase-pack/releases/download/neorunbase-archive/neorunbase-1.0.0.tar.gz\n</code></pre> <p>And untar the downloaded package. <pre><code>tar zxvf neorunbase-1.0.0.tar.gz\n\ncd neorunbase-1.0.0;\n</code></pre></p> <p>Run the example servers which are 1 Coordinator server and 2 Data nodes with Zookeeper on local.</p> <pre><code>export NEORUNBASE_MASTER_KEY=test-master-key-for-integration-tests-12345\nbin/start-example-servers.sh;\n</code></pre> <p>Environment variable <code>NEORUNBASE_MASTER_KEY</code> that must be at least 32 characters needs to be exported when running NeorunBase servers.</p> <p>After a few seconds, visit admin page of NeorunBase.</p> <pre><code>http://localhost:8080/admin\n</code></pre> <p>First initial admin user and password is <code>admin</code> / <code>admin</code>, after that you need to change the initial password.</p> <p></p>"},{"location":"intro/intro/#run-queries","title":"Run Queries","text":"<p><code>psql</code> will be used to connect to NeorunBase.</p> <pre><code>PGPASSWORD=\"&lt;your-admin-password&gt;\" psql -h localhost -p 5432 -U admin -d neorunbase\n</code></pre> <p>Run several queries like the following.</p> <pre><code>-- =============================================================================\n-- NeorunBase Quick Start Example\n--\n-- Connect:  psql -h localhost -p 5432 -U admin -d neorunbase\n-- =============================================================================\n\n\n-- ===================== 1. CREATE TABLES =====================\n\n-- Customers table\nCREATE TABLE customers (\n    id         BIGINT PRIMARY KEY,\n    name       VARCHAR(100) NOT NULL,\n    email      VARCHAR(255),\n    city       VARCHAR(50),\n    created_at TIMESTAMP DEFAULT NOW()\n) SHARD KEY (id) SHARDS 4;\n\n-- Products table\nCREATE TABLE products (\n    id       BIGINT PRIMARY KEY,\n    name     VARCHAR(200) NOT NULL,\n    category VARCHAR(50),\n    price    FLOAT NOT NULL\n) SHARD KEY (id) SHARDS 4;\n\n-- Orders table\nCREATE TABLE orders (\n    id          BIGINT PRIMARY KEY,\n    customer_id BIGINT,\n    product_id  BIGINT,\n    quantity    INT NOT NULL,\n    total       FLOAT NOT NULL,\n    status      VARCHAR(20),\n    ordered_at  TIMESTAMP DEFAULT NOW()\n) SHARD KEY (id) SHARDS 4;\n\n\n-- ===================== 2. INSERT DATA =====================\n\n-- Customers\nINSERT INTO customers (id, name, email, city) VALUES (1, 'Alice Kim',    'alice@example.com',   'Seoul');\nINSERT INTO customers (id, name, email, city) VALUES (2, 'Bob Park',     'bob@example.com',     'Busan');\nINSERT INTO customers (id, name, email, city) VALUES (3, 'Charlie Lee',  'charlie@example.com', 'Seoul');\nINSERT INTO customers (id, name, email, city) VALUES (4, 'Diana Choi',   'diana@example.com',   'Incheon');\nINSERT INTO customers (id, name, email, city) VALUES (5, 'Eve Jung',     'eve@example.com',     'Busan');\n\n-- Products\nINSERT INTO products (id, name, category, price) VALUES (1, 'NeorunBase Enterprise License', 'software', 5000.00);\nINSERT INTO products (id, name, category, price) VALUES (2, 'Premium Support (1 Year)',      'support',  2000.00);\nINSERT INTO products (id, name, category, price) VALUES (3, 'Training Workshop',             'service',  800.00);\nINSERT INTO products (id, name, category, price) VALUES (4, 'Consulting (Per Day)',           'service',  1500.00);\nINSERT INTO products (id, name, category, price) VALUES (5, 'NeorunBase Starter License',    'software', 1000.00);\n\n-- Orders\nINSERT INTO orders (id, customer_id, product_id, quantity, total, status) VALUES (1,  1, 1, 1, 5000.00,  'completed');\nINSERT INTO orders (id, customer_id, product_id, quantity, total, status) VALUES (2,  1, 2, 1, 2000.00,  'completed');\nINSERT INTO orders (id, customer_id, product_id, quantity, total, status) VALUES (3,  2, 5, 2, 2000.00,  'completed');\nINSERT INTO orders (id, customer_id, product_id, quantity, total, status) VALUES (4,  2, 3, 1, 800.00,   'pending');\nINSERT INTO orders (id, customer_id, product_id, quantity, total, status) VALUES (5,  3, 1, 1, 5000.00,  'completed');\nINSERT INTO orders (id, customer_id, product_id, quantity, total, status) VALUES (6,  3, 4, 3, 4500.00,  'completed');\nINSERT INTO orders (id, customer_id, product_id, quantity, total, status) VALUES (7,  4, 2, 1, 2000.00,  'pending');\nINSERT INTO orders (id, customer_id, product_id, quantity, total, status) VALUES (8,  4, 3, 2, 1600.00,  'completed');\nINSERT INTO orders (id, customer_id, product_id, quantity, total, status) VALUES (9,  5, 5, 1, 1000.00,  'cancelled');\nINSERT INTO orders (id, customer_id, product_id, quantity, total, status) VALUES (10, 5, 1, 1, 5000.00,  'completed');\n\n\n-- ===================== 3. SELECT QUERIES =====================\n\n-- 3-1. Basic SELECT\nSELECT * FROM customers;\n\n-- 3-2. WHERE clause\nSELECT * FROM orders WHERE status = 'completed';\n\n-- 3-3. ORDER BY with LIMIT\nSELECT * FROM orders ORDER BY total DESC LIMIT 5;\n\n-- 3-4. INNER JOIN - Order details with customer and product names\nSELECT o.id AS order_id,\n       c.name AS customer,\n       p.name AS product,\n       o.quantity,\n       o.total,\n       o.status\nFROM orders o\nINNER JOIN customers c ON o.customer_id = c.id\nINNER JOIN products p  ON o.product_id  = p.id;\n\n-- 3-5. LEFT JOIN - All customers with their order count (including those with no orders)\nSELECT c.name, COUNT(o.id)\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nGROUP BY c.name;\n\n-- 3-6. Aggregation - Total revenue by product category\nSELECT p.category, SUM(o.total), COUNT(*)\nFROM orders o\nINNER JOIN products p ON o.product_id = p.id\nWHERE o.status = 'completed'\nGROUP BY p.category;\n\n-- 3-7. Aggregation - Revenue per customer (top spenders)\nSELECT c.name, SUM(o.total), COUNT(*)\nFROM orders o\nINNER JOIN customers c ON o.customer_id = c.id\nWHERE o.status = 'completed'\nGROUP BY c.name\nORDER BY SUM(o.total) DESC;\n\n-- 3-8. HAVING - Customers who spent more than 5000\nSELECT c.name, SUM(o.total)\nFROM orders o\nINNER JOIN customers c ON o.customer_id = c.id\nWHERE o.status = 'completed'\nGROUP BY c.name\nHAVING SUM(o.total) &gt; 5000;\n\n-- 3-9. Aggregation - Revenue per city\nSELECT c.city, SUM(o.total), COUNT(*)\nFROM orders o\nINNER JOIN customers c ON o.customer_id = c.id\nWHERE o.status = 'completed'\nGROUP BY c.city\nORDER BY SUM(o.total) DESC;\n\n-- 3-10. Overall summary\nSELECT COUNT(*) AS total_orders,\n       SUM(total) AS total_revenue,\n       AVG(total) AS avg_order_value,\n       MIN(total) AS min_order,\n       MAX(total) AS max_order\nFROM orders\nWHERE status = 'completed';\n\n\n-- ===================== 4. UPDATE =====================\n\nUPDATE orders SET status = 'completed' WHERE id = 4;\n\nSELECT * FROM orders WHERE id = 4;\n\n\n-- ===================== 5. DELETE =====================\n\nDELETE FROM orders WHERE status = 'cancelled';\n\nSELECT * FROM orders ORDER BY id;\n\n\n-- ===================== 6. CLEANUP (optional) =====================\n-- Uncomment below to drop all example tables:\n-- DROP TABLE orders;\n-- DROP TABLE products;\n-- DROP TABLE customers;\n</code></pre>"},{"location":"intro/intro/#stop-example-servers","title":"Stop Example Servers","text":"<p>Stop the running example servers.</p> <pre><code>bin/stop-example-servers.sh\n</code></pre>"}]}